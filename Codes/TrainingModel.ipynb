{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "StopWords = stopwords.words('english')\n",
    "print(StopWords[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextDataSet = []\n",
    "with open('FinalDataSet.txt') as my_file:\n",
    "    for line in my_file:\n",
    "        TextDataSet.append(line.replace(\"\\n\",\"\").replace(\"\\r\",\"\"))\n",
    "my_file.close()\n",
    "\n",
    "Vectorizer = TfidfVectorizer(smooth_idf=False,stop_words=StopWords,max_df=0.63, min_df=0.001)\n",
    "DataSet = Vectorizer.fit_transform(TextDataSet)\n",
    "DataSetArray = DataSet.toarray()\n",
    "#print(Vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5582, 1393)\n"
     ]
    }
   ],
   "source": [
    "print(DataSetArray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Parameter Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "# C = 2.5\n",
    "# sigma = 0.1\n",
    "\n",
    "angry = 1002\n",
    "fear = 1002\n",
    "joy = 1002\n",
    "love = 1002\n",
    "sad = 1002\n",
    "suprise = 572"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 SVM Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KernelFunction(X1,X2,sigma):\n",
    "    # np.exp((-1)*(np.linalg.norm(X1-X2))/(2*(sigma**2)))  ## RBF Kernel Function\n",
    "    # np.dot(X1,X2)                                        ## Liner Kernel function\n",
    "    return np.exp((-1)*(np.linalg.norm(X1-X2))/(2*(sigma**2)))\n",
    "\n",
    "def KernelMatrix(Data,Y,NoOfDataSets,si):\n",
    "    K = np.ones((NoOfDataSets,NoOfDataSets))\n",
    "    for i in range(0,NoOfDataSets):\n",
    "        for j in range(0,NoOfDataSets):\n",
    "            K[i,j] = Y[i,0]*Y[j,0]*KernelFunction(Data[i,0:],Data[j,0:],si)\n",
    "    return K\n",
    "\n",
    "def SVMPara(Data,Y,C,NoOfDataSets,sigma):\n",
    "    UpperMatrix = np.concatenate((np.array([[0]]),(-1)*Y.transpose()),axis = 1)\n",
    "    LowerMatrix = np.concatenate((Y,KernelMatrix(Data,Y,NoOfDataSets,sigma) + ((1/C)*np.eye(NoOfDataSets))),axis = 1)\n",
    "    A = np.concatenate((UpperMatrix,LowerMatrix),axis = 0)\n",
    "    B = (np.concatenate((np.array([[0]]),np.array([np.ones(NoOfDataSets)])),axis = 1)).transpose()\n",
    "    return np.linalg.inv(A)@B\n",
    "\n",
    "def Weight(w,X_train,Y_train):\n",
    "    return np.concatenate((np.array([[w[0,0]]]),np.transpose(np.array([np.sum(np.multiply(np.multiply(Y_train,w[1:]),X_train),axis=0)]))),axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcomes\n",
    "###### Note: Using DatasetArray variable as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_Angry = np.array([np.concatenate(((1)*np.ones(angry),np.concatenate(((-1)*np.ones(fear ),np.concatenate(((-1)*np.ones(joy),np.concatenate(((-1)*np.ones(love),np.concatenate(((-1)*np.ones(sad),(-1)*np.ones(suprise)))))))))))]).transpose()\n",
    "S_Fear = np.array([np.concatenate(((-1)*np.ones(angry),np.concatenate(((1)*np.ones(fear ),np.concatenate(((-1)*np.ones(joy),np.concatenate(((-1)*np.ones(love),np.concatenate(((-1)*np.ones(sad),(-1)*np.ones(suprise)))))))))))]).transpose()\n",
    "S_Joy = np.array([np.concatenate(((-1)*np.ones(angry),np.concatenate(((-1)*np.ones(fear ),np.concatenate(((1)*np.ones(joy),np.concatenate(((-1)*np.ones(love),np.concatenate(((-1)*np.ones(sad),(-1)*np.ones(suprise)))))))))))]).transpose()\n",
    "S_Love = np.array([np.concatenate(((-1)*np.ones(angry),np.concatenate(((-1)*np.ones(fear ),np.concatenate(((-1)*np.ones(joy),np.concatenate(((1)*np.ones(love),np.concatenate(((-1)*np.ones(sad),(-1)*np.ones(suprise)))))))))))]).transpose()\n",
    "S_Sad = np.array([np.concatenate(((-1)*np.ones(angry),np.concatenate(((-1)*np.ones(fear ),np.concatenate(((-1)*np.ones(joy),np.concatenate(((-1)*np.ones(love),np.concatenate(((1)*np.ones(sad),(-1)*np.ones(suprise)))))))))))]).transpose()\n",
    "S_Surprise = np.array([np.concatenate(((-1)*np.ones(angry),np.concatenate(((-1)*np.ones(fear ),np.concatenate(((-1)*np.ones(joy),np.concatenate(((-1)*np.ones(love),np.concatenate(((-1)*np.ones(sad),(1)*np.ones(suprise)))))))))))]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 3\n",
      "2 4\n",
      "3 0\n",
      "3 1\n",
      "3 2\n",
      "3 3\n",
      "3 4\n",
      "4 0\n",
      "4 1\n",
      "4 2\n",
      "4 3\n",
      "4 4\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(DataSetArray,S_Sad,test_size=0.3, random_state=935)\n",
    "C = np.array([0.01, 0.1, 1, 10, 100])\n",
    "Sigma = np.array([0.001, 0.01, 0.1, 1, 10])\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0],1)),X_test), axis=1)\n",
    "Result = np.ones((len(C),len(Sigma)))\n",
    "for i in range(0,len(C)):\n",
    "    for j in range(0,len(Sigma)):\n",
    "        w = SVMPara(X_train,Y_train,C[i],X_train.shape[0],Sigma[j])\n",
    "        y = np.sign(X_test@Weight(w,X_train,Y_train))\n",
    "        Result[i,j] = np.trace(confusion_matrix(Y_test,y))/len(Y_test)\n",
    "        print(i,j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81432836 0.81432836 0.81432836 0.81432836 0.81432836]\n",
      " [0.82865672 0.82865672 0.82865672 0.82985075 0.83343284]\n",
      " [0.87164179 0.87164179 0.87164179 0.9080597  0.84238806]\n",
      " [0.84358209 0.84358209 0.84358209 0.89910448 0.8238806 ]\n",
      " [0.84179104 0.84179104 0.84179104 0.89731343 0.86985075]]\n"
     ]
    }
   ],
   "source": [
    "print(Result)\n",
    "#print(C) \n",
    "np.savetxt(\"Data_2.csv\",np.around(Result,decimals=10),delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.01 , 0.1 , 1, 10 ,100 , 1000])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bf79c014e16b3915abe4f16bab128ee50c3055dfd2d474fcf703de72c83e33a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
